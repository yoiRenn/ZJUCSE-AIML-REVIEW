[
    {
        "q": "贝叶斯学习（Bayesian Learning）的核心假定是什么？",
        "a": "待考察的量遵循某<b>概率分布</b>，且可根据这些概率及已观察到的数据进行推理，以作出最优的决策。"
    },
    {
        "q": "相比于其他非概率学习算法，贝叶斯学习方法的一个最大优点是什么？",
        "a": "观察到的每个训练样例可以<b>增量地</b>降低或升高某假设的估计概率。而其它算法通常会在某个假设与任一样例不一致时完全去掉该假设。"
    },
    {
        "q": "在贝叶斯学习中，什么是先验概率（Prior Probability）？",
        "a": "在没有训练数据前假设 $h$ 拥有的<b>初始概率</b> $P(h)$，它反映了关于 $h$ 是一正确假设的机会的背景知识。"
    },
    {
        "q": "在贝叶斯学习中，什么是后验概率（Posterior Probability）？",
        "a": "给定数据 $D$ 时假设 $h$ 成立的概率 $P(h|D)$。它是<b>先验</b>信息和<b>样本</b>信息的综合。"
    },
    {
        "q": "贝叶斯公式（Bayes Theorem）的数学表达式是什么？",
        "a": "$$P(h|D) = \\frac{<b>P(D|h)P(h)</b>}{P(D)}$$"
    },
    {
        "q": "什么是极大后验假设（MAP）？",
        "a": "在给定数据 $D$ 时，假设空间 $H$ 中可能性最大（<b>后验概率最大</b>）的假设。<br>公式：$$h_{MAP} = \\arg\\max_{h \\in H} P(D|h)P(h)$$"
    },
    {
        "q": "什么是极大似然假设（MLE）？",
        "a": "当假定 $H$ 中每个假设有<b>相同的先验概率</b>时，使 $P(D|h)$（<b>似然度</b>）最大的假设。<br>公式：$$h_{MLE} = \\arg\\max_{h \\in H} P(D|h)$$"
    },
    {
        "q": "在什么条件下，极大后验假设（MAP）等同于极大似然假设（MLE）？",
        "a": "当假设空间 $H$ 中每个假设的<b>先验概率</b> $P(h)$ 相同时。"
    },
    {
        "q": "如果训练数据是无噪声的，且目标概念包含在假设空间中，那么与数据一致的假设对应哪种贝叶斯假设？",
        "a": "每个与数据一致的假设都是 <b>MAP</b>（极大后验） 假设。"
    },
    {
        "q": "如果训练值的误差服从均值为0的独立正态分布，那么最小化误差平方和等价于寻找什么？",
        "a": "等价于寻找<b>极大似然假设</b>（MLE）。"
    },
    {
        "q": "在神经网络中，使用梯度上升法最大化 $G(h,D)$ (对数似然) 对应于最小化什么误差函数？",
        "a": "对应于最小化<b>交叉熵</b>（Cross Entropy）。<br>注：这是基于观察到的布尔值为输入实例的概率函数的前提。"
    },
    {
        "q": "什么是最小描述长度（MDL）准则？",
        "a": "选择这样的假设：它使<b>假设的描述长度</b>和<b>给定假设下数据的描述长度</b>之和最小化。<br>公式：$$h_{MDL} = \\arg\\min_{h \\in H} (L_{C_1}(h) + L_{C_2}(D|h))$$"
    },
    {
        "q": "贝叶斯最优分类器（Bayes Optimal Classifier）是如何进行分类的？",
        "a": "它将<b>所有假设</b>的预测结合起来，并用各假设的<b>后验概率</b>作为权重，计算新实例最可能的分类。<br>（注：它通常比任何单个假设的表现都要好，但计算开销大）。"
    },
    {
        "q": "什么是 Gibbs 算法？它的误差界限是多少？",
        "a": "**定义**：按照后验概率分布从 $H$ 中<b>随机选择</b>一个假设 $h$ 来预测新实例。<br>**误差界限**：在一定条件下，其期望误分类率最多为贝叶斯最优分类器的<b>两倍</b>。"
    },
    {
        "q": "朴素贝叶斯分类器（Naive Bayes Classifier）的核心假设是什么？",
        "a": "**条件独立**性假设：在给定目标值（类别）的情况下，各个属性值之间是相互独立的。"
    },
    {
        "q": "为什么朴素贝叶斯分类器要引入条件独立性假设？",
        "a": "为了解决<b>数据稀疏</b>问题（避免需要极大的训练集来估计联合概率），并降低计算复杂度。"
    },
    {
        "q": "朴素贝叶斯分类器中，为什么要使用 m-估计（m-estimate）？",
        "a": "为了解决<b>零概率</b>问题（当某个属性值在训练集中未出现时，会导致概率乘积为0）。<br>公式：$$P(w_k|v_j) = \\frac{n_k + 1}{n + |Vocabulary|}$$ （以文本分类为例，均匀先验下）。"
    },
    {
        "q": "贝叶斯信念网（Bayesian Belief Networks）主要用于解决什么问题？",
        "a": "它提供了一种中间方法，比朴素贝叶斯的<b>完全独立</b>假设限制更少（允许定义变量子集间的条件独立性），又比全联合概率计算更可行。"
    },
    {
        "q": "在贝叶斯信念网中，网络中的节点和弧分别代表什么？",
        "a": "**节点**：代表变量。<br>**弧**：代表断言“此变量在给定其直接前驱时<b>条件独立</b>于其非后继”。（即依赖关系）。"
    },
    {
        "q": "贝叶斯信念网的联合概率分布计算公式是什么？",
        "a": "$$P(y_1, ..., y_n) = \\prod_{i=1}^n P(y_i | \\text{Parents}(y_i))$$"
    },
    {
        "q": "如果贝叶斯信念网的结构已知但部分变量不可观察，通常使用什么方法来学习条件概率表？",
        "a": "<b>梯度上升</b>法（用于搜索极大似然假设）或者 <b>EM算法</b>（期望最大化）。"
    },
    {
        "q": "第五章总结：贝叶斯学习的主要知识点有哪些？",
        "a": "1. <b>基础</b>：贝叶斯公式，先验/后验概率，MAP与MLE的区别。<br>2. <b>分类器</b>：贝叶斯最优分类器（理论最优但昂贵），Gibbs算法（随机采样），朴素贝叶斯（属性独立假设，实战常用）。<br>3. <b>原理关联</b>：最小误差平方和对应高斯噪声下的MLE，最小描述长度（MDL）对应奥卡姆剃刀。<br>4. <b>扩展</b>：贝叶斯信念网（处理局部条件独立性）。"
    }
]