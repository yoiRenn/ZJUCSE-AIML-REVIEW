[
    {
        "q": "强化学习 (RL) 与监督学习的主要区别是什么？",
        "a": "1. <b>输入数据</b>：RL是<b>序列数据</b>（相互关联），监督学习通常是独立同分布数据。<br>2. <b>反馈机制</b>：RL只有<b>奖励信号</b>（标量），没有告诉智能体正确动作的标签。<br>3. <b>目标</b>：RL最大化<b>长期累积收益</b>，监督学习最小化预测误差。<br>4. <b>过程</b>：RL包含<b>探索与开发</b> (Exploration & Exploitation) 的平衡。"
    },
    {
        "q": "马尔可夫决策过程 (MDP) 的五元组通常包含哪些要素？",
        "a": "$\\langle S, A, P, R, \\gamma \\rangle$<br>S: <b>状态</b>空间<br>A: <b>动作</b>空间<br>P: <b>状态转移概率</b><br>R: <b>奖励函数</b><br>$\\gamma$: <b>衰减因子</b> (Discount factor)"
    },
    {
        "q": "在 MDP 中，什么是“马尔可夫性” (Markov Property)？",
        "a": "指未来的状态只取决于<b>当前的状态和动作</b>，而与过去的历史状态和动作<b>无关</b>。<br>即：$P(s_{t+1} | s_t, a_t, ...) = P(s_{t+1} | s_t, a_t)$"
    },
    {
        "q": "强化学习中衰减因子 $\\gamma$ (Gamma) 的作用是什么？",
        "a": "1. 数学上：避免无限时间步长的累积奖励发散（使级数<b>收敛</b>）。<br>2. 物理意义：决定智能体的“<b>远见</b>”。$\\gamma=0$ 表示只关注即时奖励（短视），$\\gamma$接近1 表示重视未来长期回报。"
    },
    {
        "q": "状态价值函数 $V_\\pi(s)$ 与 动作价值函数 $Q_\\pi(s, a)$ 的定义区别？",
        "a": "<b>$V_\\pi(s)$</b>：从<b>状态 s</b> 开始，遵循策略 $\\pi$ 能获得的预期回报。<br><b>$Q_\\pi(s, a)$</b>：从状态 s 开始，执行<b>动作 a</b>，此后遵循策略 $\\pi$ 能获得的预期回报。"
    },
    {
        "q": "贝尔曼方程 (Bellman Equation) 描述了什么关系？",
        "a": "描述了状态价值与后继状态价值之间的递归关系。根据策略不同，分为：\n1. **贝尔曼期望方程**（针对特定策略 $\\pi$）：\n$$V_\\pi(s_t) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma V_\\pi(s_{t+1})]$$\n2. **贝尔曼最优方程**（针对最优策略）：\n$$V^*(s_t) = \\max_a \\mathbb{E} [R_{t+1} + \\gamma V^*(s_{t+1})]$$\n（原答案中的公式缺失了对动作的期望或最大化操作，数学上不严谨）。"
    },
    {
        "q": "动态规划 (Dynamic Planning) 方法求解 RL 问题的最大局限性是什么？",
        "a": "需要知道完备的环境知识（即<b>模型已知</b>，已知状态转移概率 P 和奖励函数 R），这在实际问题中很难满足。"
    },
    {
        "q": "蒙特卡洛方法 (Monte Carlo, MC) 的核心思想是什么？",
        "a": "1. <b>无模型</b> (Model-free)：不需要环境的状态转移模型。<br>2. <b>基于经验</b>：通过与环境交互产生<b>完整的轨迹</b> (Episode)。<br>3. <b>更新方式</b>：利用完整轨迹的<b>平均回报</b>来估计价值函数（通常用于分幕式任务）。"
    },
    {
        "q": "时序差分学习 (TD Learning) 相比于蒙特卡洛方法 (MC) 有什么优势？",
        "a": "1. <b>在线学习</b>：不需要等到回合结束，可以<b>单步更新</b>（Bootstrapping，自举）。<br>2. <b>适用性</b>：可用于<b>连续任务</b>（非分幕式）。<br>3. <b>方差</b>：通常比 MC <b>方差更低</b>，但因自举引入了偏差。"
    },
    {
        "q": "SARSA 算法的更新公式是怎样的？它是 On-policy 还是 Off-policy？",
        "a": "公式：$Q(s, a) \\leftarrow Q(s, a) + \\alpha[R + \\gamma Q(s', a') - Q(s, a)]$<br>类型：<b>On-policy</b> (同策略)。因为它更新时使用的是<b>实际执行</b>的下一个动作 $a'$。"
    },
    {
        "q": "Q-Learning 算法的更新公式是怎样的？它是 On-policy 还是 Off-policy？",
        "a": "公式：$Q(s, a) \\leftarrow Q(s, a) + \\alpha[R + \\gamma \\max Q(s', a') - Q(s, a)]$<br>类型：<b>Off-policy</b> (异策略)。因为它更新时假设下一步采取的是<b>最优动作</b> (max)，而实际行为可能包含探索 (如 $\\epsilon$-greedy)。"
    },
    {
        "q": "解释 On-policy (同策略) 与 Off-policy (异策略) 的区别。",
        "a": "<b>On-policy</b>：采样（行为）策略与待优化（目标）策略是<b>同一个</b>。<br><b>Off-policy</b>：采样（行为）策略与待优化（目标）策略是<b>不同</b>的（例如用随机策略收集数据来优化贪婪策略）。"
    },
    {
        "q": "表格型 (Tabular) 强化学习方法（如 Q-table）的主要缺点是什么？",
        "a": "1. <b>存储限制</b>：无法处理<b>高维</b>或连续的状态空间（<b>状态爆炸</b>）。<br>2. <b>泛化能力差</b>：无法从已学习的状态推演到未见过的状态。"
    },
    {
        "q": "DQN (Deep Q-Network) 引入了哪两个关键技术来解决深度强化学习训练不稳定的问题？",
        "a": "1. <b>经验回放</b> (Experience Replay)：解决样本间的相关性问题。<br>2. <b>固定 Q 目标</b> (Fixed Q-targets)：解决目标值非平稳的问题。"
    },
    {
        "q": "在 DQN 中，经验回放 (Experience Replay) 的具体做法是什么？",
        "a": "将智能体与环境交互产生的转移样本 $(s, a, r, s')$ 存入一个<b>回放内存</b> (Replay Buffer) 中，训练时从内存中<b>随机采样</b>一个小批量 (Mini-batch) 进行梯度下降更新。"
    },
    {
        "q": "本章小结：请总结策略迭代、蒙特卡洛、TD、SARSA、Q-Learning 的分类特点。",
        "a": "1. <b>策略/价值迭代</b>：基于动态规划，<b>模型已知</b>。<br>2. <b>蒙特卡洛 (MC)</b>：无模型，需<b>完整轨迹</b>。<br>3. <b>时序差分 (TD)</b>：无模型，结合 DP 和 MC，支持<b>在线学习</b>。<br>4. <b>SARSA</b>：<b>On-policy</b> 的 TD 控制。<br>5. <b>Q-Learning</b>：<b>Off-policy</b> 的 TD 控制。<br>6. <b>DQN</b>：<b>神经网络</b>拟合 Q 值，解决高维空间问题。"
    }
]