[
    {
        "q": "汤姆·米切尔（Tom Mitchell）对机器学习的定义是什么？",
        "a": "计算机程序如何随着<b>经验积累</b>自动提高性能，系统自我改进的过程；或计算机利用经验改善系统自身性能的行为。"
    },
    {
        "q": "机器学习的核心思想是什么？",
        "a": "从数据中发现<b>模式</b> -> 做出<b>预测或决策</b>。"
    },
    {
        "q": "机器学习与传统编程的一个主要区别是什么（关于数据和规则）？",
        "a": "传统编程是显式编程；机器学习是从数据中自动学习和改进，从数据中获取知识（数据驱动决策）。"
    },
    {
        "q": "自然智慧的两个重要特点是什么（机器学习希望模仿的）？",
        "a": "1. <b>容错性</b><br>2. <b>推广能力</b>（举一反三/泛化能力）"
    },
    {
        "q": "什么是“有监督学习”（Supervised Learning）？",
        "a": "在<b>样本标签已知</b>的情况下，利用这些参数进行分类器设计。训练样本已经标注（Labeled）。<br>典型应用：分类、回归。"
    },
    {
        "q": "什么是“无监督学习”（Unsupervised Learning）？",
        "a": "在<b>无法预先知道样本标签</b>（无训练样本/Unlabeled）的情况下进行分类器设计。<br>典型应用：聚类、模式发现。"
    },
    {
        "q": "“分类”（Classification）与“聚类”（Clustering）的主要区别是什么？",
        "a": "<b>分类</b>：属于有监督学习，类别是已知的。<br><b>聚类</b>：属于无监督学习，类别是学习出来的（未知的）。"
    },
    {
        "q": "什么是“懒惰学习”（Lazy Learning）？及其主要目的？",
        "a": "<b>定义</b>：平时仅保存数据，不主动建模，直到预测时才处理数据。<br><b>目的</b>：从输入输出数据对构成的训练集中学习输入到输出的映射关系。"
    },
    {
        "q": "懒惰学习（Lazy Learning）的三个显著特征是什么？",
        "a": "1. <b>延迟学习</b>（预测时才处理，平时不建模）<br>2. <b>局部估计</b>（只依赖与查询点相关的数据）<br>3. <b>即时计算</b>（预测后不保留中间结果）"
    },
    {
        "q": "机器学习发展史：1957年谁首次提出了感知器（Perceptron）？",
        "a": "Rosenblatt"
    },
    {
        "q": "机器学习发展史：1969年《Perceptron》一书指出了感知器无法解决什么著名问题？",
        "a": "<b>XOR 问题</b>（异或问题）。这也导致了神经网络研究的第一次低潮。"
    },
    {
        "q": "机器学习发展史：1980年代，什么算法成功解决了XOR问题，标志着连接主义（神经网络）的复兴？",
        "a": "<b>MLP + BP算法</b>（多层感知机 + 反向传播算法）。"
    },
    {
        "q": "机器学习发展史：1990年代统计学习理论走向成熟的典型代表算法是什么？",
        "a": "<b>SVM</b>（支持向量机）。<br>代表人物：Vapnik。"
    },
    {
        "q": "SVM（支持向量机）基于什么原则？",
        "a": "<b>结构风险最小化</b>原则。"
    },
    {
        "q": "机器学习发展史：2012年发生的标志性事件是什么？",
        "a": "<b>深度学习</b>（Deep Learning）兴起。<br>深度学习方法在 <b>ImageNet</b> 竞赛中取得了最佳效果，显著提升了性能。"
    },
    {
        "q": "机器学习发展史：2016年，Google的什么系统战胜了李世石？",
        "a": "<b>AlphaGo</b>"
    },
    {
        "q": "机器学习受到哪些相关学科的影响？（列举3-4个）",
        "a": "1. <b>人工智能</b>（符号表示、Bayes）<br>2. <b>统计学</b>（统计学习理论 SLT）<br>3. <b>信息论</b>（最小描述长度）<br>4. <b>哲学</b>（奥卡姆剃刀、没有免费午餐）<br>5. <b>神经生物学</b>（神经网络）"
    },
    {
        "q": "在网络安全领域，入侵检测通常被看作哪种类型的机器学习问题？",
        "a": "典型的<b>预测型</b>机器学习问题（通常是有监督的分类问题，区分正常访问与入侵模式）。"
    },
    {
        "q": "Google搜索引擎的第一桶金来源于哪个基于机器学习的算法？",
        "a": "<b>PageRank</b> 算法"
    },
    {
        "q": "对于“奥卡姆剃刀”（Occam's Razor）原则在机器学习中的理解通常是什么？",
        "a": "“如无必要，勿增实体”。<br>在性能相近的模型中，通常选择<b>更简单</b>的模型（以提高泛化能力）。"
    },
    {
        "q": "机器学习中的“概念学习”（Concept Learning）是指什么？",
        "a": "从有关某个<b>布尔函数</b>的输入输出训练样例中，推断出该布尔函数。<br>（也可以看作是从特殊的训练样例中归纳出一般函数）"
    },
    {
        "q": "概念学习可以被看作是什么样的搜索过程？",
        "a": "在预定义的<b>假设空间</b>中进行搜索，寻找能够最好地拟合训练样例的假设。"
    },
    {
        "q": "在概念学习中，“归纳学习假设”（Inductive Learning Hypothesis）的内容是什么？",
        "a": "任一假设如果在足够大的训练样例集中很好地逼近目标函数，它也能在<b>未见实例</b>中很好地逼近目标函数。<br>（即：对未见实例最好的假设就是与训练数据最佳拟合的假设）"
    },
    {
        "q": "在假设空间中，什么是“一般到特殊”的序关系（More General）？",
        "a": "当且仅当对于任意实例 $x$，只要 $h_k(x)=1$ 就能推出 $h_j(x)=1$ 时，称 $h_j$ 比 $h_k$ <b>更一般</b>（或 $h_j \\ge_g h_k$）。<br>简单来说，更一般的假设覆盖了更多的正例。"
    },
    {
        "q": "假设空间上的“一般到特殊”序关系属于哪种数学关系？",
        "a": "<b>偏序关系</b>（Partial Order）。<br>（注：不是全序，因为有些假设之间无法比较谁更一般）"
    },
    {
        "q": "Find-S 算法的基本策略是什么？",
        "a": "寻找<b>极大特殊</b>假设。<br>初始化为最特殊的假设，遇到正例时，将假设进行极小一般化以覆盖该正例；遇到反例时忽略。"
    },
    {
        "q": "Find-S 算法的主要缺点有哪些？",
        "a": "1. 无法判断找到的假设是否是唯一的。<br>2. <b>容错性差</b>（对噪声敏感），若训练样例有误，结果会出错。<br>3. 无法利用反例的信息。"
    },
    {
        "q": "什么是“变型空间”（Version Space）？",
        "a": "假设空间 $H$ 中与训练样例集 $D$ <b>一致</b>的所有假设组成的子集。<br>（即：$VS_{H,D} = \\{h \\in H | Consistent(h, D)\\}$）"
    },
    {
        "q": "在变型空间中，“一致”（Consistent）的定义是什么？",
        "a": "一个假设 $h$ 与训练样例集合 $D$ 一致，当且仅当对 $D$ 中每一个样例 $<x, c(x)>$，都有 $h(x) = c(x)$。"
    },
    {
        "q": "候选消除算法（Candidate-Elimination）如何表示变型空间？",
        "a": "通过维护两个边界集合来简洁地表示：<br>1. <b>S边界</b>（极大特殊边界）：与数据一致的极大特殊成员集合。<br>2. <b>G边界</b>（极大一般边界）：与数据一致的极大一般成员集合。"
    },
    {
        "q": "在候选消除算法中，遇到“正例”时如何更新 S 和 G？",
        "a": "1. <b>S边界</b>：进行<b>极小一般化</b>（变得更一般以包含新正例），同时移除与新数据不一致的假设。<br>2. <b>G边界</b>：移除与新正例不一致的假设（变得更特殊通常不需要，除非冲突）。"
    },
    {
        "q": "在候选消除算法中，遇到“反例”时如何更新 S 和 G？",
        "a": "1. <b>S边界</b>：移除与新反例不一致的假设。<br>2. <b>G边界</b>：进行<b>极小特殊化</b>（变得更特殊以排除新反例），且需保证特殊化后的假设仍比S中的某个成员更一般。"
    },
    {
        "q": "概念学习中，最优的查询策略（选择下一个训练样例）是什么？",
        "a": "产生一个实例，使其满足当前变型空间中<b>大约半数</b>的假设。<br>这样可以将变型空间的大小减半（类似于二分查找），理论上只需 $\\log_2|VS|$ 次实验。"
    },
    {
        "q": "候选消除算法什么时候会收敛到一个空的变型空间？",
        "a": "当训练样例中<b>存在错误</b>（噪声），或者<b>目标概念无法被假设空间表示</b>（例如目标概念是析取形式，但假设空间只限于合取）时。"
    },
    {
        "q": "什么是“归纳偏置”（Inductive Bias）？",
        "a": "学习器为了从训练数据中归纳出未见实例的分类，所必须做出的<b>前提假设</b>（集合 B）。<br>如果没有偏置，学习器只能进行死记硬背（Rote Learning），无法泛化。"
    },
    {
        "q": "候选消除算法（Candidate-Elimination）的归纳偏置是什么？",
        "a": "目标概念 <b>$c$ 包含在假设空间 $H$ 中</b>。"
    },
    {
        "q": "无偏的学习器（Unbiased Learner）存在什么问题？",
        "a": "<b>无偏学习是无用的</b>。<br>如果不对目标概念的形式做任何预先假定（例如允许它是实例集的任意子集），那么它根本无法对未见实例进行分类（无法泛化）。"
    },
    {
        "q": "按照归纳偏置（Bias）的强弱，排列 Find-S、候选消除算法和机械式学习（Rote Learning）。",
        "a": "Find-S > 候选消除算法 > 机械式学习<br>（偏置越强，归纳能力越强，但也越容易因为假设错误而失败）"
    },
    {
        "q": "在 EnjoySport 例子中，如果 Sky 属性有 3 种值，其他 5 个属性各有 2 种值。语法上不同的假设有多少种？（包含 ? 和 $\\emptyset$）",
        "a": "计算公式：$(3+2) \\times (2+2)^5 = 5 \\times 4^5 = 5120$ 种。<br>（每个属性可以是特定值、? 或 $\\emptyset$）"
    },
    {
        "q": "在 EnjoySport 例子中，为什么语义上不同的假设只有 973 个？",
        "a": "因为包含 $\\emptyset$（空集/全反例）的假设在语义上是等价的（都把所有实例分为反例）。<br>计算：$1$ (全反例) $+ (3+1) \\times (2+1)^5 = 1 + 4 \\times 3^5 = 1 + 972 = 973$。"
    }
]