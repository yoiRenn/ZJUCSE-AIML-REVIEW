[
    {
        "q": "【总结】多模态大模型（MLLM）的经典架构包含哪四个核心模块？",
        "a": "1. <b>图像预处理</b>（Image Preprocessing）：如Patch Partitioning。<br>2. <b>多模态编码器</b>（Multimodal Encoder）：接收且有效编码，如CLIP。<br>3. <b>投影器</b>（Projector）：多模态数据<b>对齐</b>，映射到文本空间。<br>4. <b>大语言模型</b>（LLM Backbone）：接收对齐信号并执行推理。"
    },
    {
        "q": "多模态大模型发展经历的三个主要阶段是什么？",
        "a": "1. **第一阶段**：支持<b>低分辨率</b>（224x224），聚焦模态对齐的原型验证（如CLIP, BLIP2, LLaVA）。<br>2. **第二阶段**：增加<b>目标定位</b>能力，服务于Agent和机器人落地（如Qwen-VL）。<br>3. **第三阶段**：支持<b>高分辨率</b>文档图像，缓解语言模型能力下降问题。"
    },
    {
        "q": "什么是 **CLIP** (Contrastive Language-Image Pretraining) 的核心原理？",
        "a": "通过<b>对比学习</b>（Contrastive Learning）的方法，学习图像与文本的<b>共享嵌入空间</b>。<br>使语义相近的图像和文本在空间中靠近，语义不相关的彼此远离。"
    },
    {
        "q": "在MLLM架构中，**Projector（投影器）** 的作用是什么？列举两种常见类型。",
        "a": "**作用**：将<b>视觉嵌入</b>映射对齐到<b>文本空间</b>，使LLM能理解视觉特征。<br>**常见类型**：<br>1. <b>线性投影</b>/MLP（如LLaVA）：简单高效。<br>2. <b>Q-Former</b>（如BLIP2）：基于注意力机制，用可学习的查询向量提取特征。"
    },
    {
        "q": "多模态模型中，LLM Backbone 融合视觉特征的两种主要架构方法（Method A vs Method B）有什么区别？",
        "a": "1. **Method A（统一嵌入解码器架构）**：将图像Patch转换为Token，直接与文本Token<b>拼接</b>（Concat），作为LLM的输入（如LLaVA）。<br>2. **Method B（跨模态注意力架构）**：图像特征不直接作为Input Token，而是通过<b>交叉注意力</b>（Cross-Attention）层注入到LLM中（如Flamingo）。"
    },
    {
        "q": "什么是 **Multimodal Chain-of-Thought (CoT)**？与标准推理有何不同？",
        "a": "**定义**：基于多模态证据，在得出结论前先构建<b>中间推理步骤</b>。<br>**标准格式**：<输入 &rarr; 输出><br>**CoT格式**：<输入 &rarr; <b>推理过程</b> &rarr; 输出>"
    },
    {
        "q": "多模态智能体（Multimodal Agent）的定义及其两大分类是什么？",
        "a": "**定义**：能遵循语言指令，在环境中执行操作并调用工具的智能体。<br>**分类**：<br>1. <b>自主智能体</b>（Autonomous）：用于任务自动化，如Auto-UI, WebArena。<br>2. <b>对话智能体</b>（Conversational）：强调个性化与交互，如CAMEL, Generative Agents。"
    },
    {
        "q": "在PPT提到的 **ClevrCount** 实战任务中，使用了什么算法进行训练？奖励函数如何设计？",
        "a": "**算法**：<b>GRPO</b> (Group Relative Policy Optimization)。<br>**奖励函数**：包含两部分：<br>1. <b>格式奖励</b>（Format Reward）：确保输出符合XML标签格式。<br>2. <b>准确性奖励</b>（Accuracy Reward）：比较模型输出数字与Ground Truth是否一致。"
    },
    {
        "q": "什么是图像预处理中的 **Patch Partitioning**？",
        "a": "将图像分割成固定大小的“<b>补丁</b>”（Patches）并排成<b>序列</b>。目的是解决原始图像直接输入Transformer面临的维度过高问题，随后通常会经过<b>线性投影</b>。"
    },
    {
        "q": "列举几个音频感知（Audio Understanding）的代表性模型。",
        "a": "1. <b>SALMONN</b>：实现语音、音乐、环境音的统一理解。<br>2. <b>SpeechGPT</b>：专注于人类语音模态（ASR, TTS）。<br>3. <b>AudioGPT</b>：利用LLM作为控制器调用音频专家模型。"
    },
    {
        "q": "视频理解模型（如Video-LLaVA）相比图像模型的核心挑战是什么？",
        "a": "需要处理<b>动态时空特征</b>（Spatial-Temporal）。不仅要理解每一帧的静态内容，还要将视频的时空特征与文本语义进行高效对齐，以实现对动作、事件和因果关系的推理。"
    },
    {
        "q": "在多模态任务中，“Any-to-Any”指的是什么？",
        "a": "指大模型能够接收文本、音频、图像、视频中的<b>任意组合</b>作为<b>输入</b>，并自由生成这些模态的<b>任意组合</b>作为<b>输出</b>（如NExT-GPT）。"
    }
]