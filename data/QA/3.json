[
    {
        "q": "分类（Classification）与回归（Regression）的主要区别是什么？",
        "a": "**分类**：目标属性 $y$ 是<b>离散</b>的（类别标签）。<br>**回归**：目标属性 $y$ 是<b>连续</b>的（实数值）。"
    },
    {
        "q": "决策树中的内部节点（Internal Node）、分支（Branch）和叶子节点（Leaf Node）分别代表什么？",
        "a": "1. **内部节点**：对实例的某个<b>属性</b>的测试。<br>2. **分支**：该属性的一个可能<b>取值</b>。<br>3. **叶子节点**：实例所属的<b>分类</b>（类标号）。"
    },
    {
        "q": "从逻辑表达式的角度看，决策树代表了什么形式的规则？",
        "a": "代表了实例属性值约束的<b>合取的析取式</b>（Disjunction of Conjunctions）。<br>即：(条件A AND 条件B) OR (条件C AND 条件D)..."
    },
    {
        "q": "ID3 算法采用什么样的搜索策略？",
        "a": "<b>自顶向下</b>的<b>贪婪搜索</b>（Greedy Search），遍历可能的决策树空间。<br>（注：它不进行回溯，因此可能收敛到局部最优）。"
    },
    {
        "q": "信息熵（Entropy）的数学公式及其含义是什么？",
        "a": "公式：$Entropy(S) = - \\sum p_i \\log_2 p_i$<br>含义：度量样本集合的<b>纯度</b>或<b>不确定性</b>。<br>熵越小，集合越纯（所有样本属于同一类）；熵越大，集合越混乱。"
    },
    {
        "q": "如果一个样本集合 $S$ 中正反样例数量相等，其熵值为多少？如果所有成员属于同一类，熵值为多少？",
        "a": "1. 正反相等：Entropy(S) = <b>1</b>。<br>2. 属于同一类：Entropy(S) = <b>0</b>。"
    },
    {
        "q": "ID3 算法使用什么指标来选择最佳划分属性？",
        "a": "<b>信息增益</b> (Information Gain)。<br>它是指由于使用某个属性分割样例而导致的<b>期望熵降低</b>。"
    },
    {
        "q": "信息增益（Information Gain）准则的一个主要缺点是什么？",
        "a": "它存在内在偏置，<b>偏向于选择取值较多</b>的属性。<br>（例如：如果有一个属性是唯一的“ID号”，它的信息增益最大，但不仅没有泛化能力，还会导致过拟合）。"
    },
    {
        "q": "ID3 算法的归纳偏置（Inductive Bias）是什么？（即奥卡姆剃刀原则在决策树中的体现）",
        "a": "<b>优先选择较短的树</b>，以及将信息增益高的属性放在离根节点较近的地方。<br>（简单假设优于复杂假设）。"
    },
    {
        "q": "C4.5 算法相对于 ID3 算法做了哪些主要改进？（列举至少3点）",
        "a": "1. 使用<b>信息增益比</b>（Gain Ratio）代替信息增益，克服偏向多值属性的缺点。<br>2. 能够处理<b>连续值</b>属性（离散化）。<br>3. 能够处理<b>缺失值</b>。<br>4. 引入了<b>剪枝</b>（Pruning）机制以防止过拟合。"
    },
    {
        "q": "信息增益比（Gain Ratio）是如何修正信息增益的？",
        "a": "它在信息增益的基础上除以了<b>分裂信息</b>（SplitInformation），即引入了对属性分裂广度和均匀性的惩罚项。"
    },
    {
        "q": "CART 算法的全称是什么？它构建的是什么类型的树？",
        "a": "**CART** = Classification and Regression Trees（分类与回归树）。<br>它构建的是<b>二叉树</b>（Binary Tree）。"
    },
    {
        "q": "CART 算法在分类任务和回归任务中分别使用什么指标作为划分标准？",
        "a": "1. **分类**：使用<b>基尼指数</b> (Gini Index)。<br>2. **回归**：使用<b>均方误差</b> (MSE) 或平方误差和。"
    },
    {
        "q": "基尼指数（Gini Index）的公式是什么？它反映了什么？",
        "a": "公式：$Gini(S) = 1 - \\sum p_i^2$<br>含义：反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。<b>基尼值越小，纯度越高</b>。"
    },
    {
        "q": "什么是决策树的“过拟合”（Overfitting）？",
        "a": "假设 $h$ 在训练集上的表现很好（错误率低），但在新的<b>测试集</b>（整个实例分布）上的表现比另一个假设 $h'$ 差。即模型把训练数据中的<b>噪声</b>或巧合规律也学习了，导致泛化能力下降。"
    },
    {
        "q": "决策树防止过拟合的两种主要策略是什么？",
        "a": "1. <b>预剪枝</b> (Pre-pruning)：在树生长过程中及早停止（如限制深度、样本数）。<br>2. <b>后剪枝</b> (Post-pruning)：先让树充分生长，然后自底向上修剪掉无助于提高验证集精度的分支。"
    },
    {
        "q": "C4.5 算法如何处理连续值属性？",
        "a": "采用<b>二分法</b>。<br>将连续属性的取值排序，取相邻两个值的<b>中点</b>作为候选分裂点（阈值），计算每个分裂点的信息增益，选择最优的阈值将连续属性离散化为二元属性。"
    },
    {
        "q": "在决策树学习中，如何处理缺失属性值的训练样例？（列举常见策略）",
        "a": "1. 赋给它当前节点训练样例中该属性的<b>最常见值</b>。<br>2. 赋给它当前节点属于同一类别（$y$）的训练样例中该属性的最常见值。<br>3. 为该属性的每个可能值赋予一个<b>概率权重</b>（C4.5采用的方法）。"
    },
    {
        "q": "决策树的“规则后修剪”（Rule Post-Pruning）的基本过程是什么？",
        "a": "1. 允许决策树过度拟合。<br>2. 将树转化为<b>等价的规则集合</b>（每条路径一条规则）。<br>3. 修剪规则中不会导致精度降低的前件（Preconditions）。<br>4. 对规则集进行排序应用。"
    }
]