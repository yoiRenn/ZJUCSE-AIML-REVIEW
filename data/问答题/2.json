[
    {
        "q": "线性模型（Linear Model）的一般向量形式是什么？",
        "a": "$$f(x) = <b>w^T x + b</b>$$<br>其中 $w$ 为<b>权重</b>向量，$b$ 为<b>偏置</b>。"
    },
    {
        "q": "线性回归中，如何处理“无序”的离散属性？",
        "a": "将其转化为 <b>k维向量</b>（通常使用<b>独热编码</b> One-hot encoding）。<br>例如：颜色={红, 黄, 蓝} -> (0,0,1), (0,1,0)..."
    },
    {
        "q": "线性回归最常用的参数估计方法是什么？",
        "a": "<b>最小二乘法</b> (Least Square Method)。<br>目标是最小化<b>均方误差</b>（MSE）。"
    },
    {
        "q": "当数据矩阵 $X^TX$ 不满秩或样本量极大时，通常使用什么方法求解线性回归参数？",
        "a": "<b>梯度下降</b>法 (Gradient Descent)。"
    },
    {
        "q": "什么是广义线性模型 (Generalized Linear Model)？",
        "a": "$$y = g^{-1}(w^T x + b)$$<br>其中 $g(\\cdot)$ 称为<b>联系函数</b> (Link Function)，它将线性模型的预测值与真实标记联系起来。"
    },
    {
        "q": "什么是岭回归 (Ridge Regression)？其核心特点是什么？",
        "a": "在平方误差的基础上增加了 <b>L2范数</b>正则化项 ($|w|_2^2$)。<br>特点：收缩系数，处理<b>多重共线性</b>，但不会将系数压缩为0。"
    },
    {
        "q": "什么是 Lasso 回归？其核心特点是什么？",
        "a": "在平方误差的基础上增加了 <b>L1范数</b>正则化项 ($|w|_1$)。<br>特点：能获得<b>稀疏解</b>（将部分系数压缩为0），因此可用于<b>特征选择</b>。"
    },
    {
        "q": "用形象的比喻描述岭回归 (Ridge) 和 Lasso 的区别？",
        "a": "<b>岭回归</b>像公平的管理者：每个部门（特征）预算都<b>削减一点</b>，不完全砍掉。<br><b>Lasso</b>像果断的决策者：保留关键部门，不重要的直接<b>砍掉</b>（系数置0）。"
    },
    {
        "q": "什么是弹性网络 (Elastic Net)？",
        "a": "结合了 <b>L1</b> (Lasso) 和 <b>L2</b> (Ridge) 两种正则化项的模型。<br>既具有<b>稀疏性</b>，又具有<b>稳定性</b>。"
    },
    {
        "q": "为什么不能直接用线性回归 (Linear Regression) 做分类任务？",
        "a": "1. <b>输出范围</b>不匹配：回归输出是连续值，分类需要离散值或概率。<br>2. 对<b>异常值敏感</b>：均方误差对远离边界的点惩罚过大，导致决策边界偏移。"
    },
    {
        "q": "对数几率回归 (Logistic Regression) 使用什么函数将线性输出映射为概率？",
        "a": "<b>Sigmoid</b> 函数 (或对数几率函数)：<br>$$y = \\frac{1}{1 + e^{-z}}$$ "
    },
    {
        "q": "对数几率回归 (Logistic Regression) 通常使用什么方法进行参数估计？",
        "a": "<b>极大似然法</b> (Maximum Likelihood Estimation)，即最小化负对数似然函数。"
    },
    {
        "q": "Softmax 回归适用于什么场景？",
        "a": "适用于<b>多分类</b>问题。<br>它是逻辑回归在多分类上的推广。"
    },
    {
        "q": "感知机 (Perceptron) 的激活函数是什么？",
        "a": "<b>符号函数</b> (Sign function)：<br>输出 +1 或 -1。"
    },
    {
        "q": "单层感知机 (Perceptron) 的最大局限性是什么？",
        "a": "只能解决<b>线性可分</b>问题，无法解决 <b>异或</b> (XOR) 等非线性问题。"
    },
    {
        "q": "线性判别分析 (LDA) 的核心思想是什么（一句话概括）？",
        "a": "<b>最大化类间散度</b>，<b>最小化类内散度</b>。<br>（让同类样例尽可能接近，异类样例尽可能远离）。"
    },
    {
        "q": "LDA (线性判别分析) 除了用于分类，还可以被视为一种什么技术？",
        "a": "一种<b>监督降维</b>技术。"
    },
    {
        "q": "在多分类学习中，常用的拆分策略有哪些？",
        "a": "1. <b>一对一</b> (OvO)：训练 $N(N-1)/2$ 个分类器，投票决定。<br>2. <b>一对其余</b> (OvR/OvA)：训练 $N$ 个分类器，取置信度最高的。"
    },
    {
        "q": "欠拟合 (Underfitting) 的主要原因及解决办法？",
        "a": "<b>原因</b>：<b>特征维度过少</b>，模型太简单。<br><b>解决</b>：增加特征维度，引入<b>非线性特征</b>。"
    },
    {
        "q": "过拟合 (Overfitting) 的主要原因及解决办法？",
        "a": "<b>原因</b>：特征维度过多，模型太复杂，拟合了<b>噪声</b>。<br><b>解决</b>：减少特征、<b>正则化</b> (Ridge/Lasso)、数据集扩增、早停等。"
    },
    {
        "q": "什么是基函数扩展 (Basis Function Expansion)？",
        "a": "通过基函数 $\\phi(x)$ 将原始特征映射到<b>更高维</b>空间，在新空间中用线性模型解决原空间的<b>非线性</b>问题。"
    },
    {
        "q": "解决类别不平衡 (Class Imbalance) 问题的三种常见方法？",
        "a": "1. <b>再缩放</b>/阈值移动 (Rescaling)。<br>2. <b>欠采样</b> (Undersampling)。<br>3. <b>过采样</b> (Oversampling)。"
    },
    {
        "q": "SMOTE 算法的基本原理是什么？",
        "a": "属于<b>过采样</b>方法。<br>不是简单复制样本，而是在少数类样本之间进行<b>线性插值</b>生成新样本，以避免过拟合。"
    }
]