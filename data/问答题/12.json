[
    {
        "q": "什么是对抗搜索 (Adversarial Search)？",
        "a": "指在<b>竞争环境</b>中，多个<b>智能体</b>（Agent）之间的目标是有<b>冲突</b>的（如博弈游戏），智能体需要根据当前局势选择对自己最有利的行动的搜索问题。"
    },
    {
        "q": "在博弈论中，什么是纳什均衡 (Nash Equilibrium)？",
        "a": "指博弈中这样一种<b>稳定的状态</b>：如果任意一位参与者在其他所有参与者的策略确定的情况下，其选择的策略是<b>最优</b>的（即没有人愿意<b>单方面改变</b>自己的策略），那么这个组合就被定义为纳什均衡。"
    },
    {
        "q": "极大极小算法 (Minimax Algorithm) 的核心假设是什么？",
        "a": "假设对手是<b>完全理性</b>的，即对手每一步都会做出对其最有利（对你最不利）的选择（\"料敌先机\"）。"
    },
    {
        "q": "在极大极小算法中，MAX节点和MIN节点分别如何选择回传值？",
        "a": "<b>MAX节点</b>：选择子节点中的<b>最大值</b>（目标是最大化自己的收益）。<br><b>MIN节点</b>：选择子节点中的<b>最小值</b>（目标是最小化对手的收益）。"
    },
    {
        "q": "$\\alpha-\\beta$ 剪枝算法的主要作用和特点是什么？",
        "a": "<b>作用</b>：是极大极小算法的<b>优化</b>，通过<b>剪去</b>不必要的搜索分支来提高效率。<br><b>特点</b>：它产生的结果与 Minimax <b>完全相同</b>，但运行效率更高。"
    },
    {
        "q": "在 $\\alpha-\\beta$ 剪枝中，$\\alpha$ 和 $\\beta$ 分别代表什么含义？",
        "a": "<b>$\\alpha$</b>：到目前为止路径上发现的 <b>MAX</b> 的最佳选择（下界，极大值）。<br><b>$\\beta$</b>：到目前为止路径上发现的 <b>MIN</b> 的最佳选择（上界，极小值）。"
    },
    {
        "q": "$\\alpha-\\beta$ 剪枝发生的条件是什么？",
        "a": "当 <b>$\\alpha \\ge \\beta$</b> 时，该节点剩余的分支就不必继续搜索了（发生剪枝）。"
    },
    {
        "q": "在资源受限（如时间有限）的情况下，如何改进完全的极大极小搜索？",
        "a": "使用<b>截断搜索</b> (Cutoff Search)：<br>1. 用<b>启发式评估函数</b> (Evaluation Function) 代替效用函数。<br>2. 用<b>截断测试</b>（如深度限制）取代终止测试。"
    },
    {
        "q": "常用的线性加权评估函数的形式是怎样的？",
        "a": "$$Eval(s) = \\sum_{i=1}^{n} w_i f_i(s)$$<br>其中 $w_i$ 是<b>权值</b>，$f_i(s)$ 是棋局的某个<b>特征</b>（如棋子数量、价值等）。"
    },
    {
        "q": "什么是“水平线效应” (Horizon Effect)？",
        "a": "指由于搜索<b>深度有限</b>，AI 无法看到截断点（“水平线”）之后即将发生的<b>灾难性后果</b>（如丢子），从而做出误判。"
    },
    {
        "q": "为了缓解水平线效应，通常采用什么技术？",
        "a": "<b>静止搜索</b> (Quiescence Search)：在到达预设深度时，如果局面处于“<b>动荡</b>”状态（如吃子、将军），则继续搜索直到局面恢复“静止”才调用评估函数。"
    },
    {
        "q": "随机博弈 (Stochastic Game) 与马尔可夫决策过程 (MDP) 的主要区别是什么？",
        "a": "MDP 是<b>单智能体</b>、多状态的决策过程；<br>随机博弈是<b>多智能体</b>、多状态的博弈过程。"
    },
    {
        "q": "在多智能体强化学习中，为什么独立使用 Q-learning 会遇到困难？",
        "a": "因为每个智能体都在更新策略，导致对于单个智能体而言，<b>环境是非固定的</b> (Non-stationary)，这违反了 Q-learning 的基本假设。"
    },
    {
        "q": "纳什 Q-learning (Nash Q-learning) 的基本思想是什么？",
        "a": "在每个状态下，求解当前阶段的<b>纳什均衡</b>策略，并使用纳什均衡的值来更新 Q 函数。"
    },
    {
        "q": "本章小结：请总结对抗搜索的核心知识体系。",
        "a": "1. <b>基础</b>：博弈分类、纳什均衡。<br>2. <b>算法</b>：Minimax（完全理性假设）、$\\alpha-\\beta$ 剪枝（$\\alpha \\ge \\beta$ 剪枝）、截断搜索（评估函数）。<br>3. <b>优化</b>：静止搜索（解决水平线效应）。<br>4. <b>拓展</b>：多智能体博弈（随机博弈、纳什 Q-learning）。"
    }
]