[
    {
        "q": "【总结】第十五章 大模型推理技术的核心脉络是什么？",
        "a": "本章主要分为三个部分：<br>1. <b>推理及优化</b>：关注如何解决大模型部署的“显存受限”和“串行延迟”瓶颈。核心技术包括<b>模型压缩</b>（量化、剪枝、蒸馏）和系统级优化（KV Cache、运行时批处理）。<br>2. <b>生成式推理策略</b>：关注解码过程（Decoding）。从确定性策略（贪心、束搜索）演变为随机采样（Top-k, Top-p），以及前沿的<b>推测性解码</b>。<br>3. <b>提示工程</b>：关注如何通过Prompt激活模型潜能。核心技术包括<b>思维链</b>（CoT）及其变体、<b>ReAct</b>（推理+行动）范式。"
    },
    {
        "q": "大模型中 Inference（推理/推断）与 Reasoning（推理/逻辑）的区别是什么？",
        "a": "1. <b>Inference</b> (推断/应用)：指模型训练完成后，<b>部署</b>并处理新数据以生成预测的过程。关注效率、速度（工程侧）。<br>2. <b>Reasoning</b> (逻辑推理)：指模型内在的<b>思考</b>、分析复杂问题的能力（如CoT）。关注智能表现、中间思维步骤（算法侧）。"
    },
    {
        "q": "大模型推理面临的“三重瓶颈”是什么？",
        "a": "1. <b>巨大的模型尺寸</b>：海量参数导致显存占用极高（Memory Bound）。<br>2. <b>自回归解码的串行性</b>：逐Token生成，无法并行，延迟随序列长度线性增长。<br>3. <b>自注意力机制的二次复杂度</b>：计算量与序列长度N的<b>平方</b>（$O(N^2)$）成正比。"
    },
    {
        "q": "为什么说大模型推理的深层本质是“内存受限”（Memory Bound）？",
        "a": "尽管GPU计算能力强大，但在LLM推理中，计算核心大部分时间都在等待数据从<b>高带宽内存 (HBM)</b> 传输。每生成一个Token都需要<b>加载全部模型参数</b>和KV缓存，导致计算单元闲置。"
    },
    {
        "q": "量化（Quantization）的核心原理和优势是什么？",
        "a": "**原理**：降低模型参数和激活值的<b>数值精度</b>（如FP16转INT8/INT4）。<br>**优势**：<br>1. 显著减小模型体积，降低<b>显存</b>需求。<br>2. 在相同内存带宽下传输更多参数，提升<b>推理速度</b>。"
    },
    {
        "q": "结构化剪枝 vs 非结构化剪枝",
        "a": "1. <b>非结构化剪枝</b>：移除<b>单个参数</b>，造成不规则稀疏。硬件难以利用。<br>2. <b>结构化剪枝</b>：移除整个<b>神经元</b>、注意力头或层。<b>硬件友好</b>，可直接转化为更小的矩阵计算，实现加速。"
    },
    {
        "q": "知识蒸馏（Knowledge Distillation）在LLM中的应用？",
        "a": "将庞大复杂的“<b>教师模型</b>”的知识迁移到更小、更高效的“<b>学生模型</b>”中。<br>（例如：DeepSeek-R1验证了通过数据蒸馏将大模型能力迁移到小模型）。"
    },
    {
        "q": "KV缓存（KV-Cache）的作用是什么？",
        "a": "**作用**：解决自回归解码中的<b>重复计算</b>问题。<br>**原理**：在“解码”阶段，只计算新Token的Q、K、V，并<b>复用</b>缓存中的旧K、V。<br>**代价**：以<b>显存换速度</b>（显存消耗随序列长度增加）。"
    },
    {
        "q": "运行时批处理（In-Flight Batching）解决了什么问题？",
        "a": "解决了传统批处理需要等待同批次中最长序列完成的问题。它允许服务器在运行时立即<b>移除已完成</b>的序列，并<b>加入新请求</b>，大幅提高GPU利用率和吞吐量。"
    },
    {
        "q": "贪心搜索（Greedy Search）与束搜索（Beam Search）的区别？",
        "a": "1. <b>贪心搜索</b>：每步只选概率<b>最高</b>的1个Token。易陷入局部最优。<br>2. <b>束搜索</b>：每步维护 <b>K个</b> (Beam Width) 最优候选序列。具有前瞻性，生成质量更高。"
    },
    {
        "q": "Top-k 采样 vs Top-p (核) 采样",
        "a": "1. <b>Top-k</b>：仅从概率最高的 <b>K个</b> Token中随机选择。缺点是K值固定。<br>2. <b>Top-p</b> (Nucleus)：从<b>累积概率</b>达到阈值 P 的动态集合中选择。更灵活，平衡了多样性和准确性。"
    },
    {
        "q": "推测性解码（Speculative Decoding）的工作原理是什么？",
        "a": "利用“大模型+小模型”协同：<br>1. <b>草稿生成</b>：小模型快速生成多个候选Token。<br>2. <b>并行验证</b>：大模型<b>并行验证</b>这些草稿。<br>优势：利用大模型并行计算能力，显著降低延迟。"
    },
    {
        "q": "为什么中间Token（思维链）对Transformer推理至关重要？",
        "a": "Transformer通过生成中间Token（思维链），实际上增加了逻辑电路的深度（<b>时间换空间</b>）。这使得固定大小的模型能够解决需要更多计算步骤的复杂问题。"
    },
    {
        "q": "什么是思维链（CoT）及其核心作用？",
        "a": "**定义**：引导模型将复杂问题分解为一系列<b>中间推理步骤</b>。<br>**作用**：显著提升模型在多步推理任务上的准确性。CoT是大模型达到一定规模后<b>涌现</b>的能力。"
    },
    {
        "q": "Zero-shot CoT 的经典提示词是什么？",
        "a": "<b>Let's think step by step.</b> （让我们逐步思考。）"
    },
    {
        "q": "ReAct (Reason+Act) 框架的核心思想是什么？",
        "a": "结合了<b>推理</b> (Reasoning) 和<b>行动</b> (Acting)。<br>让模型交错生成推理轨迹（思考下一步做什么）和具体动作（调用工具/API），从而解决需要与<b>外部环境交互</b>的复杂任务。"
    },
    {
        "q": "推理性能评估的关键指标有哪些？",
        "a": "1. <b>延迟</b> (Latency)：包括 <b>TTFT</b> (首字生成时间) 和 <b>ITL</b> (Token间延迟)。<br>2. <b>吞吐量</b> (Throughput)：单位时间处理的Token数。<br>3. <b>成本</b>：每百万Token的费用。"
    },
    {
        "q": "常见的LLM评估基准（Benchmark）有哪些？",
        "a": "1. <b>MMLU</b>：通用知识和推理。<br>2. <b>HumanEval</b>：<b>代码</b>生成能力。<br>3. <b>TruthfulQA</b>：评估真实性，检测“<b>幻觉</b>”。"
    }
]