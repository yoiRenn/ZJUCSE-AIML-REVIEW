[
    {
        "q": "人工神经元模型（Artificial Neuron）主要由哪几个部分组成？",
        "a": "1. <b>输入</b> ($x$)：对应生物神经元的树突。<br>2. <b>权值</b> ($w$)：对应突触的连接强度。<br>3. <b>加权求和</b> ($\\sum$)：对应细胞体的信息累积。<br>4. <b>激活</b>/输出函数 ($f$)：对应阈值判断和脉冲输出。"
    },
    {
        "q": "感知器（Perceptron）的定义及其输出规则是什么？",
        "a": "感知器是一个<b>线性分类</b>模型。<br>它计算输入的线性组合，如果结果大于某个阈值，输出 <b>1</b>，否则输出 <b>-1</b>。<br>公式：$o(\\vec{x}) = \\text{sgn}(\\vec{w} \\cdot \\vec{x})$"
    },
    {
        "q": "感知器（Perceptron）无法解决哪类著名的逻辑问题？为什么？",
        "a": "无法解决 <b>异或</b> (XOR) 问题。<br>原因：感知器本质上是一个<b>线性分类器</b>（超平面），而 XOR 问题是<b>线性不可分</b>的。"
    },
    {
        "q": "感知器训练法则（Perceptron Training Rule）的权值更新公式是什么？",
        "a": "$$w_i \\leftarrow w_i + \\eta (t - o) x_i$$<br>其中 $\\eta$ 是<b>学习率</b>，$t$ 是目标值，$o$ 是实际输出，$x_i$ 是输入。"
    },
    {
        "q": "感知器训练法则能够收敛的前提条件是什么？",
        "a": "1. 训练样例是<b>线性可分</b>的。<br>2. <b>学习率</b> $\\eta$ 足够小。"
    },
    {
        "q": "什么是 Delta 法则（Delta Rule）？它与感知器法则的主要区别是什么？",
        "a": "Delta 法则使用<b>梯度下降</b>（Gradient Descent）来最小化误差平方和。<br><b>区别</b>：Delta 法则即使在数据<b>线性不可分</b>时也能收敛到目标概念的最佳近似（最小误差），而感知器法则无法保证。"
    },
    {
        "q": "在神经网络训练中，标准梯度下降（Batch）与随机梯度下降（SGD）的主要区别是什么？",
        "a": "**标准梯度下降**：在更新权值前对<b>所有</b>样例汇总误差。计算量大，但步长可稍大。<br>**随机梯度下降**：每处理<b>一个</b>样例就更新一次权值。计算快，可能避免<b>局部极小值</b>，但路径震荡。"
    },
    {
        "q": "为什么多层前馈网络中通常使用 Sigmoid 单元而不是感知器单元（阶跃函数）？",
        "a": "因为感知器的阶跃函数是不连续、<b>不可微</b>的，无法应用基于梯度的优化算法（如反向传播）。<br>Sigmoid 函数是平滑、连续且<b>处处可导</b>的。"
    },
    {
        "q": "反向传播算法（BP）的核心思想是什么？",
        "a": "利用<b>链式法则</b>，将输出层的误差<b>反向传播</b>回隐藏层，从而计算出每个权值对总误差的贡献（<b>梯度</b>），并据此调整权值。"
    },
    {
        "q": "在反向传播中，输出单元 $k$ 的误差项 $\\delta_k$ 如何计算？（Sigmoid激活函数）",
        "a": "$$\\delta_k = o_k(1 - o_k)(t_k - o_k)$$<br>包含三部分：输出值的<b>导数</b> $o_k(1-o_k)$ 和 预测<b>误差</b> $(t_k - o_k)$。"
    },
    {
        "q": "在反向传播中，隐藏层单元 $h$ 的误差项 $\\delta_h$ 是如何得到的？",
        "a": "通过对受该隐层单元影响的所有下游单元的误差 $\\delta_k$ 进行<b>加权求和</b>（权重为 $w_{kh}$），再乘在该隐层单元的激活函数<b>导数</b>。<br>即：利用下游误差来“归咎”上游节点。"
    },
    {
        "q": "在梯度下降中加入“冲量项”（Momentum）有什么作用？",
        "a": "1. 像球滚下山坡一样，利用惯性冲过狭窄的<b>局部极小值</b>或平坦区域。<br>2. 在梯度不变区域增大搜索步长，<b>加快收敛</b>。<br>公式：$\\Delta w(n) = \\eta \\delta x + \\alpha \\Delta w(n-1)$"
    },
    {
        "q": "多层前馈神经网络（Multi-layer Feedforward NN）的表征能力如何？（通用近似定理）",
        "a": "1. <b>布尔函数</b>：两层网络可表示任意布尔函数。<br>2. <b>连续函数</b>：包含一个隐层的网络可以以任意精度<b>逼近</b>任何有界连续函数。"
    },
    {
        "q": "反向传播算法的归纳偏置（Inductive Bias）是什么？",
        "a": "倾向于在数据点之间进行<b>平滑插值</b>。<br>（即给定两个正例，倾向于把中间的点也标记为正例）。"
    },
    {
        "q": "神经网络训练中，为什么会出现“过度拟合”（Overfitting）？现象是什么？",
        "a": "<b>原因</b>：权值迭代次数过多，模型变得过于复杂，拟合了数据中的<b>噪声</b>或特异性特征。<br><b>现象</b>：训练集误差持续下降，但<b>验证集误差</b>先下降后上升。"
    },
    {
        "q": "解决神经网络过拟合的常用方法有哪些？（至少列举2种）",
        "a": "1. <b>权值衰减</b>（Weight Decay）：在误差函数中加入权值大小的惩罚项。<br>2. <b>早停法</b>（Early Stopping）：使用验证集，当验证误差开始上升时停止训练。<br>3. <b>交叉验证</b>：确定最佳的隐层单元数或迭代次数。"
    },
    {
        "q": "循环神经网络（RNN）与前馈神经网络最大的区别是什么？",
        "a": "RNN 具有<b>记忆</b>（Memory）。<br>RNN 的层内、层与层之间有反馈连接，当前时刻的输出不仅取决于当前输入，还取决于<b>上一时刻</b>的隐状态。适合处理<b>序列数据</b>（如NLP）。"
    },
    {
        "q": "LSTM（长短期记忆网络）主要是为了解决 RNN 的什么问题？",
        "a": "解决简单 RNN 在处理长序列时出现的<b>梯度消失</b>（或梯度爆炸）问题，使其能够捕捉<b>长期依赖</b>（Long-term dependencies）关系。"
    },
    {
        "q": "在人脸识别案例中，为什么输出层采用 1-of-n 编码（如4个输出单元）而不是单个输出单元？",
        "a": "1. 为网络表示目标函数提供更大的<b>自由度</b>。<br>2. 输出值的分布（如0.9, 0.1, 0.1, 0.1）可以表示分类的<b>置信度</b>。"
    },
    {
        "q": "在人脸识别案例中，为什么目标值设为 <0.9, 0.1...> 而不是 <1, 0...>？",
        "a": "因为 Sigmoid 函数仅在输入趋于无穷大时才输出 1 或 0。<br>如果强行拟合 1 或 0，会导致<b>权值无限增长</b>，引发过拟合或梯度问题。"
    }
]