id,type,gender,word,cn,forms,example,source
701,填空,,输入数据; 反馈机制; 目标; 过程,强化学习 (RL) 与监督学习的主要区别：1. ____：RL是序列数据，监督学习是独立同分布数据。; 2. ____：RL只有奖励信号，没有正确动作标签。; 3. ____：RL最大化长期累积收益，监督学习最小化预测误差。; 4. ____：RL包含探索与开发的平衡。,完整：1. 输入数据... 2. 反馈机制... 3. 目标... 4. 过程...,Chapter7_RL,Anki
702,填空,,⟨S, A, P, R, γ⟩,马尔可夫决策过程 (MDP) 的五元组通常包含：____; S: 状态空间; A: 动作空间; P: 状态转移概率; R: 奖励函数; γ: 衰减因子。,完整：⟨S, A, P, R, γ⟩...,Chapter7_RL,Anki
703,填空,,策略 (Policy, π); 价值函数 (Value Function); 模型 (Model),强化学习Agent的三个主要组成部分：1. ____：从状态到动作的映射。; 2. ____：预测未来累积奖励的期望。; 3. ____：对环境状态转移和奖励的预测。,完整：1. 策略... 2. 价值函数... 3. 模型...,Chapter7_RL,Anki
704,填空,,确定性策略; 随机策略,策略 $\pi$ 的两种主要类型：1. ____ ($a=\pi(s)$)。; 2. ____ ($\pi(a|s) = P[A_t=a|S_t=s]$)。,完整：1. 确定性策略... 2. 随机策略...,Chapter7_RL,Anki
705,填空,,状态价值函数; 动作价值函数 (Q函数),价值函数的两种定义：1. ____ $v_\pi(s)$：从状态 s 出发遵循策略 $\pi$ 的期望回报。; 2. ____ $q_\pi(s, a)$：从状态 s 出发，执行动作 a，之后遵循策略 $\pi$ 的期望回报。,完整：1. 状态价值函数... 2. 动作价值函数...,Chapter7_RL,Anki
706,填空,,贝尔曼方程 (Bellman Equation),描述当前状态价值与后继状态价值之间关系的方程是____。,完整：贝尔曼方程 (Bellman Equation),Chapter7_RL,Anki
707,填空,,无模型 (Model-free); 有模型 (Model-based),根据是否建立环境模型，RL 算法可分为：1. ____：直接从与环境交互的经验中学习策略或价值函数（如 Q-learning）。; 2. ____：先学习环境的状态转移和奖励模型，再基于模型进行规划。,完整：1. 无模型... 2. 有模型...,Chapter7_RL,Anki
708,填空,,基于价值 (Value-based); 基于策略 (Policy-based); Actor-Critic,根据学习目标的不同，RL 算法可分为：1. ____：学习价值函数，隐式推导策略（如 DQN）。; 2. ____：直接优化策略参数（如 Policy Gradient）。; 3. ____：同时学习策略（Actor）和价值函数（Critic）。,完整：1. 基于价值... 2. 基于策略... 3. Actor-Critic,Chapter7_RL,Anki
709,填空,,利用 (Exploitation); 探索 (Exploration),探索与利用 (Exploration vs. Exploitation) 困境：____：执行当前已知的最优动作以获得最大短期奖励。; ____：尝试未执行过的动作以获取更多信息，可能发现更优策略。,完整：利用 (Exploitation)... 探索 (Exploration)...,Chapter7_RL,Anki
710,填空,,ε-greedy,解决探索与利用困境的常用简单策略是____策略（以 $\epsilon$ 概率随机探索，以 $1-\epsilon$ 概率选择当前最优）。,完整：ε-greedy 策略,Chapter7_RL,Anki