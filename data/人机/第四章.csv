id,type,gender,word,cn,forms,example,source
401,填空,,输入; 权值; 加权求和; 激活/输出函数,人工神经元模型主要组成：1. ____($x$); 2. ____($w$); 3. ____($\sum$); 4. ____($f$)。,完整：输入、权值、加权求和、激活函数,Chapter4,Anki
402,填空,,1; -1,感知器（Perceptron）输出规则：如果结果大于阈值输出____，否则输出____。,完整：1；-1,Chapter4,Anki
403,填空,,异或; 线性分类器; 线性不可分,感知器无法解决____问题：因为它是____，而该问题是____的。,完整：异或；线性分类器；线性不可分,Chapter4,Anki
404,填空,,w_i + \eta (t - o) x_i,感知器训练法则权值更新公式：w_i \leftarrow ____。,完整：w_i + \eta (t - o) x_i,Chapter4,Anki
405,填空,,线性可分; 足够小,感知器收敛前提：1. 训练样例____; 2. 学习率____。,完整：线性可分；足够小,Chapter4,Anki
406,填空,,梯度下降; 线性不可分,Delta 法则：使用____最小化误差；与感知器法则区别在于即使____也能收敛到最佳近似。,完整：梯度下降；线性不可分,Chapter4,Anki
407,填空,,所有; 一个,标准梯度下降（Batch）与随机梯度下降（SGD）：Batch对____样例汇总误差；SGD每处理____样例更新。,完整：所有；一个,Chapter4,Anki
408,填空,,不可微; 处处可导,多层前馈网络使用 Sigmoid 而非阶跃函数：因为阶跃函数____；Sigmoid ____。,完整：不可微；处处可导,Chapter4,Anki
409,填空,,链式法则; 梯度,反向传播算法（BP）核心思想：利用____计算每个权值对总误差的____。,完整：链式法则；梯度,Chapter4,Anki
410,填空,,o_k(1 - o_k)(t_k - o_k),反向传播输出单元误差项 $\delta_k$：____。,完整：o_k(1 - o_k)(t_k - o_k),Chapter4,Anki
411,填空,,加权求和,反向传播隐藏层单元误差项 $\delta_h$：对下游单元误差进行____，再乘激活函数导数。,完整：加权求和,Chapter4,Anki
412,填空,,局部极小值; 加快收敛,梯度下降加入“冲量项”（Momentum）作用：1. 冲过____; 2. ____。,完整：局部极小值；加快收敛,Chapter4,Anki
413,填空,,布尔函数; 连续函数,多层前馈神经网络表征能力：两层网络可表示任意____；含隐层网络可逼近任意____。,完整：布尔函数；连续函数,Chapter4,Anki
414,填空,,平滑插值,反向传播算法归纳偏置：倾向于在数据点之间进行____。,完整：平滑插值,Chapter4,Anki
415,填空,,噪声; 先下降后上升,神经网络过拟合：拟合了____；现象是验证集误差____。,完整：噪声；先下降后上升,Chapter4,Anki
416,填空,,权值衰减; 早停法; 交叉验证,解决神经网络过拟合方法：1. ____; 2. ____; 3. ____。,完整：权值衰减；早停法；交叉验证,Chapter4,Anki
417,填空,,记忆; 上一时刻的隐状态,RNN 与前馈神经网络最大区别：RNN 具有____；输出取决于当前输入和____。,完整：记忆；上一时刻的隐状态,Chapter4,Anki
418,填空,,梯度消失; 长期依赖,LSTM 解决 RNN 的____问题，使其能捕捉____关系。,完整：梯度消失；长期依赖,Chapter4,Anki
419,填空,,自由度; 置信度,人脸识别输出层采用 1-of-n 编码原因：1. 提供更大____; 2. 分布可表示____。,完整：自由度；置信度,Chapter4,Anki
420,填空,,权值无限增长,人脸识别目标值设为 <0.9, 0.1> 而非 <1, 0>：防止____。,完整：权值无限增长,Chapter4,Anki